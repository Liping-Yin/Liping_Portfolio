{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: ML- Decision Trees\n",
    "\n",
    "\n",
    "The goal of this practical/lab is to get you acquainted with how decision trees work.   \n",
    "We will look at how to calculate entropy and information gain, and then write functions to get the feature that maximizes information gain, and use it to split our dataset.   \n",
    "We will load a naive implementation of ID3 algorithm and train it on our sample dataset, and then compare its performance to that of SKlearn's tree classifier.  \n",
    "(students interested in a more challenging exercise can provide their own implementation of ID3, this should be slightly easier than normal given you already implemented the necessary constituents of the algorithm).  \n",
    "We'll first start by importing the necessary libraries for the exercises.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then load the same dataset used in this week's tutorial (from exercises 1 and 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>a3</td>\n",
       "      <td>b1</td>\n",
       "      <td>c1</td>\n",
       "      <td>d1</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>b1</td>\n",
       "      <td>c1</td>\n",
       "      <td>d1</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>a3</td>\n",
       "      <td>b1</td>\n",
       "      <td>c1</td>\n",
       "      <td>d1</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a1</td>\n",
       "      <td>b1</td>\n",
       "      <td>c1</td>\n",
       "      <td>d1</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>a1</td>\n",
       "      <td>b1</td>\n",
       "      <td>c2</td>\n",
       "      <td>d1</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A   B   C   D variety\n",
       "29  a3  b1  c1  d1       y\n",
       "0   a1  b1  c1  d1       y\n",
       "27  a3  b1  c1  d1       y\n",
       "9   a1  b1  c1  d1       y\n",
       "61  a1  b1  c2  d1       n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the dataset from week 4 tutorial\n",
    "dataset=pd.read_csv(\"dataset.csv\")\n",
    "dataset=shuffle(dataset)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Entropy \n",
    "\n",
    "In this exercise, we will look at how to measure entropy in a set/subset, which we will later reuse to calculate the information gain of each single feature.  \n",
    "We defined entropy in the lecture as a measure of uncertainty or chaos, and is given by the below equation.\n",
    "## $ Entropy = -\\sum_{e=1}^{n} (P(e)*log_2(P(e))) $  \n",
    "\n",
    "**Question 1:**   \n",
    "Write a function that takes as a parameter a label column (in our example that's \"variety\") and returns the entropy of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y    50\n",
      "n    40\n",
      "Name: variety, dtype: int64\n",
      "['y' 'n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def entropy(label):\n",
    "    #Insert your code below\n",
    "    \n",
    "    probabilities= label.value_counts(normalize=True) # alternative: counts = np.bincount(label)\n",
    "    entropy_value = 0\n",
    "    for p in probabilities:\n",
    "        if p > 0:\n",
    "            entropy_value += - p*np.log2(p)\n",
    "        \n",
    "    return entropy_value\n",
    "\n",
    "print(dataset.variety.value_counts()) # tests\n",
    "print(pd.unique(dataset.variety)) # tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  \n",
    "Use the function you defined above to confirm the values you calculated in exercise 1 of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910760598382222\n"
     ]
    }
   ],
   "source": [
    "#Insert your code here\n",
    "#0.9910760598382222\n",
    "\n",
    "print(entropy(dataset.variety))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Information Gain\n",
    "\n",
    "As discussed in the lecture, information gain measures the decrease in entropy after a dataset is split on a feature.\n",
    "Let's proceed now to calculate the information gain of a dataset when splitting using a split_feature.\n",
    "\n",
    "## $ information\\_gain(S,A) = Entropy(S)-\\sum_{i=1}^{n}*(\\frac{|Si|}{|S|}*Entropy(S))$\n",
    "**Question 1:**  \n",
    "Write a function that takes as parameters:\n",
    "\n",
    "1. dataset\n",
    "2. label\n",
    "3. feature\n",
    "\n",
    "Your function information_gain should make use of the function entropy which you defined in exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(dataset, label, feature):\n",
    "    \"\"\"\n",
    "    Inser your code here\n",
    "    \"\"\"\n",
    "    information_gain = 0\n",
    "    # for every attribute , calculated the weighted entropy of each sub_features     \n",
    "    sub_features = pd.unique(feature) # find all the sub_features\n",
    "\n",
    "    temp_df = pd.DataFrame()\n",
    "    feature_entropy = 0\n",
    "    for f in sub_features:\n",
    "        temp_df = dataset.loc[feature == f]\n",
    "        p = (feature.values == f).sum()/len(feature)\n",
    "        sub_feature_entropy = entropy(temp_df.variety)# each entropy of sub_features respectively\n",
    "        feature_entropy += p*sub_feature_entropy\n",
    "    \n",
    "    information_gain = entropy(label) - feature_entropy\n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  \n",
    "Use the function you defined above to calculate the information gain of feature/attribute A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15809587018183202\n"
     ]
    }
   ],
   "source": [
    "#Insert your code here\n",
    "#0.15809587018183202\n",
    "label = dataset.variety\n",
    "fA = dataset.A\n",
    "info_gain_VA = information_gain(dataset,label,fA)\n",
    "print(info_gain_VA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Splitting Datasets Based on Information Gain\n",
    "We split a dataset based on the feature which brings the highest information gain. Your task in this exercise is to first find the attribute that maximizes the information gain, then split the dataset based on it.   \n",
    "**Question 1:**  \n",
    "Write a function **feature_to_split_on(dataset)** which takes an argument **dataset** and returns the name of the attribute upon which the dataset will be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_to_split_on(dataset):\n",
    "    '''\n",
    "    Insert your code here\n",
    "    '''\n",
    "    info_gains = []\n",
    "    \n",
    "    for feature in dataset.loc[:,dataset.columns != 'variety' ].columns:\n",
    "        info_gain = information_gain(dataset, dataset.variety, dataset[feature])\n",
    "        info_gains.append(info_gain)\n",
    "        \n",
    "    max_info = np.max(info_gains)\n",
    "    index = np.where(info_gains==max_info)\n",
    "    feature = dataset.columns[index][0]\n",
    "\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  \n",
    "Check that your function is working correctly by trying it on the dataset loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "#Insert your code here\n",
    "#A\n",
    "print(feature_to_split_on(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:**  \n",
    "Finally write a function **split_dataset(dataset,feature)** which takes as parameters:  \n",
    "1. dataset\n",
    "2. feature (the feature upon which we will split the dataset)  \n",
    "Your function should return an array of subdatasets (in the form of dataframes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, feature):\n",
    "    '''\n",
    "    Insert your code here\n",
    "    \n",
    "    '''\n",
    "    return list(dataset.groupby(feature))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:**  \n",
    "Check that your function is working by running it on dataset and feature \"A\". This should return 3 subdatasets where the values for \"A\" are not crosscuting. If you split on \"B\" you should only get 2 subdatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a1',\n",
       "       A   B   C   D variety\n",
       "  0   a1  b1  c1  d1       y\n",
       "  9   a1  b1  c1  d1       y\n",
       "  61  a1  b1  c2  d1       n\n",
       "  74  a1  b2  c2  d2       n\n",
       "  59  a1  b1  c2  d1       n\n",
       "  10  a1  b1  c1  d1       y\n",
       "  37  a1  b2  c1  d2       n\n",
       "  8   a1  b1  c1  d1       y\n",
       "  34  a1  b2  c1  d2       n\n",
       "  58  a1  b1  c2  d1       n\n",
       "  36  a1  b2  c1  d2       n\n",
       "  41  a1  b2  c1  d2       n\n",
       "  43  a1  b2  c1  d2       n\n",
       "  39  a1  b2  c1  d2       n\n",
       "  6   a1  b1  c1  d1       y\n",
       "  11  a1  b1  c1  d1       y\n",
       "  12  a1  b1  c1  d1       n\n",
       "  33  a1  b2  c1  d2       n\n",
       "  40  a1  b2  c1  d2       n\n",
       "  14  a1  b1  c1  d1       n\n",
       "  76  a1  b2  c2  d2       n\n",
       "  57  a1  b1  c2  d1       n\n",
       "  5   a1  b1  c1  d1       y\n",
       "  7   a1  b1  c1  d1       y\n",
       "  15  a1  b1  c1  d1       n\n",
       "  56  a1  b1  c2  d1       n\n",
       "  35  a1  b2  c1  d2       n\n",
       "  1   a1  b1  c1  d1       y\n",
       "  75  a1  b2  c2  d2       n\n",
       "  4   a1  b1  c1  d1       y\n",
       "  38  a1  b2  c1  d2       n\n",
       "  60  a1  b1  c2  d1       n\n",
       "  32  a1  b2  c1  d2       n\n",
       "  42  a1  b2  c1  d2       n\n",
       "  2   a1  b1  c1  d1       y\n",
       "  55  a1  b1  c2  d1       n\n",
       "  3   a1  b1  c1  d1       y\n",
       "  13  a1  b1  c1  d1       n\n",
       "  77  a1  b2  c2  d2       n\n",
       "  54  a1  b1  c2  d1       n),\n",
       " ('a2',\n",
       "       A   B   C   D variety\n",
       "  20  a2  b1  c1  d2       n\n",
       "  48  a2  b2  c1  d1       n\n",
       "  85  a2  b2  c2  d1       y\n",
       "  25  a2  b1  c1  d2       n\n",
       "  82  a2  b2  c2  d1       y\n",
       "  83  a2  b2  c2  d1       y\n",
       "  68  a2  b1  c2  d2       y\n",
       "  66  a2  b1  c2  d2       y\n",
       "  65  a2  b1  c2  d2       y\n",
       "  69  a2  b1  c2  d2       y\n",
       "  80  a2  b2  c2  d1       y\n",
       "  67  a2  b1  c2  d2       y\n",
       "  62  a2  b1  c2  d2       y\n",
       "  49  a2  b2  c1  d1       n\n",
       "  21  a2  b1  c1  d2       n\n",
       "  84  a2  b2  c2  d1       y\n",
       "  45  a2  b2  c1  d1       y\n",
       "  79  a2  b2  c2  d1       y\n",
       "  64  a2  b1  c2  d2       y\n",
       "  24  a2  b1  c1  d2       n\n",
       "  63  a2  b1  c2  d2       y\n",
       "  19  a2  b1  c1  d2       y\n",
       "  47  a2  b2  c1  d1       y\n",
       "  81  a2  b2  c2  d1       y\n",
       "  46  a2  b2  c1  d1       y\n",
       "  16  a2  b1  c1  d2       y\n",
       "  17  a2  b1  c1  d2       y\n",
       "  78  a2  b2  c2  d1       y\n",
       "  18  a2  b1  c1  d2       y\n",
       "  44  a2  b2  c1  d1       y\n",
       "  22  a2  b1  c1  d2       n\n",
       "  23  a2  b1  c1  d2       n),\n",
       " ('a3',\n",
       "       A   B   C   D variety\n",
       "  29  a3  b1  c1  d1       y\n",
       "  27  a3  b1  c1  d1       y\n",
       "  26  a3  b1  c1  d1       y\n",
       "  50  a3  b2  c1  d2       n\n",
       "  72  a3  b1  c2  d1       y\n",
       "  52  a3  b2  c1  d2       n\n",
       "  53  a3  b2  c1  d2       n\n",
       "  87  a3  b2  c2  d2       y\n",
       "  30  a3  b1  c1  d1       y\n",
       "  51  a3  b2  c1  d2       n\n",
       "  88  a3  b2  c2  d2       y\n",
       "  28  a3  b1  c1  d1       y\n",
       "  73  a3  b1  c2  d1       y\n",
       "  86  a3  b2  c2  d2       y\n",
       "  71  a3  b1  c2  d1       y\n",
       "  89  a3  b2  c2  d2       y\n",
       "  70  a3  b1  c2  d1       y\n",
       "  31  a3  b1  c1  d1       y)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert code here\n",
    "\n",
    "split_dataset(dataset,'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Iterative Dichotomiser 3 (ID3)\n",
    "\n",
    "ID3 is one of the simplest algorithms used to learn decision trees from data. \n",
    "Study the pseudo-code below (from wikipedia):\n",
    "\n",
    "> \n",
    "    ID3 (Examples, Target_Attribute, Attributes)  \n",
    "        Create a root node for the tree  \n",
    "        If all examples are positive, Return the single-node tree Root, with label = +.  \n",
    "        If all examples are negative, Return the single-node tree Root, with label = -.  \n",
    "        If number of predicting attributes is empty, then Return the single node tree Root,  \n",
    "        with label = most common value of the target attribute in the examples.  \n",
    "        Otherwise Begin  \n",
    "            A ← The Attribute that best classifies examples.  \n",
    "            Decision Tree attribute for Root = A.  \n",
    "            For each possible value, vi, of A,  \n",
    "                Add a new tree branch below Root, corresponding to the test A = vi.  \n",
    "                Let Examples(vi) be the subset of examples that have the value vi for A  \n",
    "                If Examples(vi) is empty  \n",
    "                    Then below this new branch add a leaf node with label = most common target value in the examples  \n",
    "                Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})  \n",
    "        End  \n",
    "        Return Root  \n",
    "\n",
    "\n",
    "You can, optionally, implement your own function **decision_tree_id3(subdataset,dataset,label,features,parent_node)** which requires the following parameters:  \n",
    "1. dataset\n",
    "2. subdataset (in your first iteration, dataset and subdataset are equal)\n",
    "3. label (what you're trying to predict)\n",
    "4. features (what you'll be splitting your dataset/subdataset on)\n",
    "5. parent_node  \n",
    "\n",
    "Alternatively, we use a naive implementation which we import from Iterative_Dichotomiser3.   \n",
    "The algorithm has been trained on all the dataset as you can see below, leaving no space for us to assess how it performs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Iterative_Dichotomiser3 as ID\n",
    "\n",
    "tree=ID.ID3(dataset,dataset,dataset.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**  \n",
    "Write a function **train_test_split(dataset, ratio)** which takes a dataset as an input and returns two datasets one for training and another for testing.\n",
    "For our example dataset, we have 90 rows and so calling your function with the parameters (dataset, 0.1) will return a training set with 81 rows and a testing set with 9 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, ratio):\n",
    "    '''\n",
    "    Insert your code here\n",
    "    '''\n",
    "    training_data = dataset.sample(frac=1-ratio)\n",
    "    testing_data = dataset.drop(training_data.index)\n",
    "    \n",
    "#     row_count = dataset.shape[0]\n",
    "#     split_point = int(row_count*(1-ratio))\n",
    "#     training_data,testing_data = dataset[:split_point], dataset[split_point:]\n",
    "    return training_data,testing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  \n",
    "Test whether your function produces the two subdatasets correctly. Print the shape of the test set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 5)\n"
     ]
    }
   ],
   "source": [
    "#Insert code here\n",
    "#(9,5)\n",
    "# print(train_test_split(dataset,0.1)[0])\n",
    "# print(train_test_split(dataset,0.1)[1])\n",
    "print(train_test_split(dataset,0.1)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally train the ID3 tree classifier on the training set and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26    y\n",
      "83    y\n",
      "80    y\n",
      "51    n\n",
      "75    n\n",
      "28    y\n",
      "46    y\n",
      "70    y\n",
      "54    n\n",
      "Name: variety, dtype: object\n",
      "{'A': {'a1': {'B': {'b1': {'C': {'c1': {'D': {'d1': 'y'}}, 'c2': 'n'}}, 'b2': 'n'}}, 'a2': {'C': {'c1': {'B': {'b1': {'D': {'d2': 'n'}}, 'b2': {'D': {'d1': 'y'}}}}, 'c2': 'y'}}, 'a3': {'B': {'b1': 'y', 'b2': {'C': {'c1': 'n', 'c2': 'y'}}}}}}\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Too many indexers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2607c36752e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariety\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mID\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariety\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#The prediction accuracy is:  77.77777777777779 %\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Iterative_Dichotomiser3.pyc\u001b[0m in \u001b[0;36mtest\u001b[1;34m(data, tree)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    871\u001b[0m                     \u001b[1;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    874\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1444\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Too many indexers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexingError\u001b[0m: Too many indexers"
     ]
    }
   ],
   "source": [
    "\n",
    "train = train_test_split(dataset,0.1)[0]\n",
    "test = train_test_split(dataset,0.1)[1]\n",
    "tree = ID.ID3(train,train,train.columns[:-1])\n",
    "print(test.variety)\n",
    "print(tree)\n",
    "ID.test(test.variety,tree)\n",
    "\n",
    "#The prediction accuracy is:  77.77777777777779 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Compairing your Decision Tree with SKlearn's\n",
    "\n",
    "\n",
    "**Question 1:**  \n",
    "  \n",
    "Use the **DecisionTreeClassifier** available in the **sklearn** library to train a decision tree model on the example dataset we've been working on so far.    \n",
    "\n",
    "Fine-tune the tree classifier as you see fit and make sure it uses **entropy** not **gini**.       \n",
    "**Some data preprocessing might be required before you can run the classifier on the example dataset.**    \n",
    "Check the accuracy of the sklearn classifier and compare it to the one we imported from Iterative_Dichotomiser3 (which we partly implemented).    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "#Insert your code here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_features = pd.get_dummies(train.drop('variety',axis=1))\n",
    "train_target = train['variety']\n",
    "test_features = pd.get_dummies(test.drop('variety',axis=1))\n",
    "test_target = test['variety']\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion = 'entropy',max_depth=3 ).fit(train_features,train_target) \n",
    "predict = tree.predict(test_features)\n",
    "\n",
    "print(\"The prediction accuracy is: \",accuracy_score(test_target, predict)*100,\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:**  \n",
    "Give an interpretation of why your classifier underperforms (or outperforms if you didn't fine-tune the sklearn claffier well) compared to the sklearn tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your answer here\n",
    "# on the choice of splitter: string, optional (default=”best”), since our dataset has only a few features and possiblity withut overfitting problem, so use default is safer\n",
    "# max_depth: int or None, optional (default=None): if the data is overfitting, the accuracy might low, so decrease the max_depth can increase the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
