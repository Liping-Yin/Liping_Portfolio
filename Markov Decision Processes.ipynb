{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    
    "First, implement Value iteration to compute the utility \n",
    "of the cells in a grid world. Let's first start by setting up the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04  0.   -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gamma = 1                # The discount factor\n",
    "backgroundReward = -0.04    # Reward of the non-terminal states\n",
    "width = 4                   # Width of the grid\n",
    "height = 3                  # Height of the grid\n",
    "\n",
    "# We're working in \"x,y\" coordinates, and address these as such in the \n",
    "# numpy matrices. M[x,y] So the matrices are stored with the width rows\n",
    "# and height columns!\n",
    "\n",
    "blockedCells = [(1,1)]         # Cells that are not accessible\n",
    "terminalCells = [(3,0),(3,1)]  # Terminal cells...\n",
    "terminalRewards = [1,-1]       # ... and the corresponding rewards\n",
    "\n",
    "# Initialise a matrix containing the rewards:\n",
    "reward=0.\n",
    "def initRewards(backgroundReward):\n",
    "    global reward\n",
    "    reward = np.zeros((width,height)) + backgroundReward\n",
    "    for c,r in zip(terminalCells,terminalRewards):\n",
    "        reward[c]=r\n",
    "    for c in blockedCells:\n",
    "        reward[c]=0 # Set the reward in the blocked cell to zero, for later convenience\n",
    "initRewards(backgroundReward)\n",
    "print(reward.T) # Print the transposed matrix for correct visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, implement some functions to evaluate if \n",
    "given coordinates correspond to a valid cell, what the coordinates\n",
    "would be if we performed a certain action (up, left, down, right) with\n",
    "no stochasticity but taking the walls into account, and a pretty-printing function for a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def valid(x,y):\n",
    "    \"\"\"Return whether (x,y) is a valid grid position\"\"\"\n",
    "    return x >= 0 and x < width and y >= 0 and y < height and (x,y) not in blockedCells\n",
    "\n",
    "def doAction(x,y,a):\n",
    "    \"\"\"What is the result of action a in cell (x,y) in a deterministic world\"\"\"\n",
    "    if (x,y) in terminalCells or (x,y) in blockedCells:\n",
    "        return blockedCells[0]  # Where the reward is zero\n",
    "    cur = (x,y)\n",
    "\n",
    "    \n",
    "    if a == 0: # north\n",
    "        y -= 1\n",
    "    elif a == 1: # east\n",
    "        x += 1\n",
    "    elif a == 2: # south\n",
    "        y += 1;\n",
    "    elif a == 3: # west\n",
    "        x -= 1\n",
    "\n",
    "    if valid(x,y):\n",
    "        return (x,y)\n",
    "\n",
    "    return cur\n",
    "\n",
    "def printPolicy(policy):\n",
    "    \"\"\"Print a policy table, that is, an action for each possible state,\n",
    "    in human-readable format\"\"\"\n",
    "    actions = [\n",
    "        \"Up    \",\n",
    "        \"Right \",\n",
    "        \"Down  \",\n",
    "        \"Left  \",\n",
    "        \"      \", # -1, no action\n",
    "    ]\n",
    "    for x in range(policy.shape[0]):\n",
    "        for y in range(policy.shape[1]):\n",
    "            print(actions[policy[x,y]],end=\"\")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Recall that value iteration updates the utilities as follows:\n",
    "\n",
    "$$\n",
    "U_i(s) \\leftarrow R(s) + \\gamma \\max_{a\\in A(s)} \\sum_{s'} P(s'\\mid s,a) \\, U_{i-1}(s')\n",
    "$$\n",
    "\n",
    "Implement a function, which we will call ```bestValue(x,y,utility)``` which takes as arguments\n",
    "- the coordinates x,y of the current cell\n",
    "- utility, the table of the \"old\" estimates of the utility values, $U_{i-1}$\n",
    "\n",
    "and returns\n",
    "- The action $a$ for which the highest expected value is obtained, and\n",
    "- the expected value of that action: $\\sum_{s'} P(s'\\mid s,a) \\, U_{i-1}(s')$\n",
    "\n",
    "For cells where no action is possible (terminal states), we return no action (index -1) and zero value. We use the transition probabilities from the book ($0.8$ for the intended actions, and $0.1$ for the two perpendicular motions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(reward[0,0])\n",
    "\n",
    "def bestValue(x,y,utility):\n",
    "    \"\"\"Return the best action we can take in cell x,y, and its value\"\"\"\n",
    "    bestUtility = float('-inf')\n",
    "    currentUtility = float('-inf')\n",
    "    action = 0\n",
    "    if (x,y) in terminalCells or (x,y) in blockedCells:\n",
    "        return -1,0.0\n",
    "    else:\n",
    "        for i in range(4):\n",
    "            currentUtility = 0.8*utility[doAction(x,y,i)] + 0.1*utility[doAction(x,y,((i-1) % 4))] + 0.1*utility[doAction(x,y,((i+1) % 4))]\n",
    "            if (currentUtility > bestUtility):\n",
    "                bestUtility = currentUtility\n",
    "                action = i\n",
    "    #return (action, round(bestUtility, 3))\n",
    "    return action,bestUtility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\## Question 2\n",
    "\n",
    "Using the above function, implement a function ```valueIteration```, which implements a single iteration of the value iteration algorithm. It takes the current utility table as input, and returns:\n",
    "- the new estimate of the utility table\n",
    "- the corresponding policy, that is, the table of best actions for each grid location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def valueIteration(utility):\n",
    "    policy = np.zeros((width,height), dtype=int)\n",
    "    newUtility = utility.copy()\n",
    "    \"\"\"Fill in the next utility table ut with the updated \n",
    "    values from the previous utility table ut1, and store the \n",
    "    best action for each cell in the policy table\"\"\"\n",
    "    # Your code comes here\n",
    "    \n",
    "    for i in range(width):\n",
    "        for y in range(height):\n",
    "            newUtility[i][y]=(gamma*bestValue(i,y,utility)[1])+reward[i][y]\n",
    "            policy[i][y]=(bestValue(i,y,utility)[0])\n",
    "    return newUtility,policy\n",
    "#printPolicy(valueIteration(utility)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "The following code uses above implementated functions to implement value iteration. For each iteration, it stores the largest change in utility in the \"delta\" list, and stops if the list hasn't changed yet.\n",
    "\n",
    "- How fast does the algorithm converge for different values of $\\gamma$? (For example, $\\gamma=.1$? What is it for $\\gamma=.9$, $\\gamma=1$)\n",
    "\n",
    "\n",
    "- Is the utility the same for these different values of $\\gamma$?\n",
    "\n",
    "\n",
    "- Is the resulting policy the same for these different values of $\\gamma$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for gamma =  1  : 60\n",
      "-0.04\n",
      "Number of iterations for gamma =  1  : 60\n",
      "-0.04\n",
      "[[ 0.81155822  0.86780822  0.91780822  1.        ]\n",
      " [ 0.76155822  0.          0.66027397 -1.        ]\n",
      " [ 0.70530822  0.65530822  0.61141553  0.38792491]] \n",
      " [[ 1  1  1 -1]\n",
      " [ 0 -1  0 -1]\n",
      " [ 0  3  3  3]] \n",
      "\n",
      "Number of iterations for gamma =  0.9  : 49\n",
      "-0.04\n",
      "Number of iterations for gamma =  0.9  : 49\n",
      "-0.04\n",
      "[[ 0.5094156   0.64958636  0.79536224  1.        ]\n",
      " [ 0.39851125  0.          0.48644046 -1.        ]\n",
      " [ 0.29646654  0.25396055  0.3447884   0.12994247]] \n",
      " [[ 1  1  1 -1]\n",
      " [ 0 -1  0 -1]\n",
      " [ 0  1  0  3]] \n",
      "\n",
      "Number of iterations for gamma =  0.1  : 19\n",
      "-0.04\n",
      "Number of iterations for gamma =  0.1  : 19\n",
      "-0.04\n",
      "[[-0.04388718 -0.03755393  0.03996438  1.        ]\n",
      " [-0.04439895  0.         -0.04352616 -1.        ]\n",
      " [-0.04444071 -0.04443844 -0.04437091 -0.04444364]] \n",
      " [[ 1  1  1 -1]\n",
      " [ 0 -1  3 -1]\n",
      " [ 0  1  0  2]]\n"
     ]
    }
   ],
   "source": [
    "def iterate():\n",
    "    utility = np.zeros((width,height))\n",
    "    delta = []\n",
    "    while len(delta) < 5 or delta[-2]-delta[-1] > 0.0:\n",
    "        nextut, policy = valueIteration(utility)\n",
    "        delta.append((nextut-utility).max())\n",
    "        #print(nextut.T)\n",
    "        #printPolicy(policy.T)\n",
    "        utility = nextut.copy()\n",
    "    print(\"Number of iterations for gamma = \",gamma,\" :\",len(delta))\n",
    "    print(backgroundReward)\n",
    "    return delta, utility.T, policy.T\n",
    "\n",
    "print(iterate()[1],'\\n',iterate()[2],'\\n')\n",
    "gamma = 0.9\n",
    "print(iterate()[1],'\\n',iterate()[2],'\\n')\n",
    "gamma = 0.1\n",
    "print(iterate()[1],'\\n',iterate()[2])\n",
    "\n",
    "# As gamma decreases the number iterations also decrease\n",
    "\n",
    "# The utility changes since the number of iteration has decreased resulting in values smaller than the ones\n",
    "#with gamma 1. When gamma is 0.1 the utility of most cells is negative\n",
    "\n",
    "# The policies are also different. As gamma decreases the agent chooses shorter paths and when gamma is 0.1 it sometimes chooses \n",
    "# an invalid cell because the utility of that cell is 0.\n",
    "\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Print out the policies you get with the following rewards for non-terminal states (with $\\gamma=1$):\n",
    "- -2\n",
    "- -1\n",
    "- -0.4\n",
    "- -0.01\n",
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for gamma =  1  : 5\n",
      "-2\n",
      "[[ 1  1  1 -1]\n",
      " [ 0 -1  1 -1]\n",
      " [ 1  1  1  0]]\n"
     ]
    }
   ],
   "source": [
    "backgroundReward = -2\n",
    "initRewards(backgroundReward)\n",
    "print(iterate()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for gamma =  1  : 47\n",
      "-1\n",
      "[[ 1  1  1 -1]\n",
      " [ 0 -1  0 -1]\n",
      " [ 1  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "backgroundReward = -1\n",
    "initRewards(backgroundReward)\n",
    "print(iterate()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for gamma =  1  : 54\n",
      "-0.4\n",
      "[[ 1  1  1 -1]\n",
      " [ 0 -1  0 -1]\n",
      " [ 0  1  0  3]]\n"
     ]
    }
   ],
   "source": [
    "backgroundReward = -0.4\n",
    "initRewards(backgroundReward)\n",
    "print(iterate()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for gamma =  1  : 309\n",
      "-0.01\n",
      "[[ 1  1  1 -1]\n",
      " [ 0 -1  3 -1]\n",
      " [ 0  3  3  2]]\n"
     ]
    }
   ],
   "source": [
    "backgroundReward = -0.01\n",
    "initRewards(backgroundReward)\n",
    "print(iterate()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for gamma =  1  : 5\n",
      "1\n",
      "[[ 0  0  3 -1]\n",
      " [ 0 -1  3 -1]\n",
      " [ 0  0  0  2]]\n"
     ]
    }
   ],
   "source": [
    "backgroundReward = 1\n",
    "initRewards(backgroundReward)\n",
    "print(iterate()[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
